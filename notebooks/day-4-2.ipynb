{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6f6ed6-79e9-4efc-836a-040000ce8c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# day 4-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b43ff",
   "metadata": {},
   "source": [
    "このノートブックの実行例は[こちら(HTML版)](../notebooks-sample/day-4-2.html)で確認できます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7303c7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9bc8c-8096-4d76-b983-250b7a4b8203",
   "metadata": {},
   "source": [
    "## 0. はじめに"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794a09-51b9-449b-b699-32a6318de81b",
   "metadata": {},
   "source": [
    "ページ上部のメニューバーにある **Kernel** メニューをクリックし、プルダウンメニューから [**Change Kernel ...**] を選び、**gssm2024:Python** を選択してください。\n",
    "\n",
    "<img src=\"images/change_kernel1.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097064cc-d520-4893-a0e6-16ff5b34c15a",
   "metadata": {},
   "source": [
    "ノートブック上部の右隅に表示されたカーネル名が **gssm2024:Python** になっていることを確認してください。\n",
    "\n",
    "<img src=\"images/change_kernel2.png\" width=\"30%\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038e0af-8843-493e-831f-2bceee5b6656",
   "metadata": {},
   "source": [
    "## 1. テキスト分析 (実践編)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e3e63",
   "metadata": {},
   "source": [
    "### 1.0 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad7a6b",
   "metadata": {},
   "source": [
    "#### 1.0.1 定義済み関数の読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb3e5b",
   "metadata": {},
   "source": [
    "以下のセルを**修正せず**に実行してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b83452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import gssm_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867f595",
   "metadata": {},
   "source": [
    "#### 1.0.1 データのダウンロード (前回ダウンロード済みのためスキップ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368d5e8",
   "metadata": {},
   "source": [
    "以下のデータがダウンロード済みです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27502dbc",
   "metadata": {},
   "source": [
    "| ファイル名 | 件数 | データセット | 備考 |\n",
    "| --- | --- | --- | --- |\n",
    "| rakuten-1000-2023-2024.xlsx.zip | 10,000 | •レジャー+ビジネスの 10エリア<br>•エリアごと 1,000件 (ランダムサンプリング)<br>•期間: 2023/1~2024 GW明け | 本講義の全体を通して使用する |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# もし、再度ダウンロードが必要な場合は残りの行のコメントマーク「#」を除去して、このセルを再実行してください\n",
    "\n",
    "# FILE_ID = \"1EeCuDrfKdlsMxG9p3Ot7TIxfV9_f2smY\"\n",
    "# !gdown --id {FILE_ID}\n",
    "# !unzip -o rakuten-1000-2023-2024.xlsx.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba6d12",
   "metadata": {},
   "source": [
    "#### 1.0.2 データの読み込み (DataFrame型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_df = pd.read_excel(\"rakuten-1000-2023-2024.xlsx\")\n",
    "print(all_df.shape)\n",
    "display(all_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c983a2",
   "metadata": {},
   "source": [
    "#### 1.0.3 「文書-抽出語」表 を作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99c1ac",
   "metadata": {},
   "source": [
    "コメント列から単語を抽出する (単語を品詞「名詞」「形容詞」「未知語」で絞り込む)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f750bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from collections import defaultdict\n",
    "import MeCab\n",
    "\n",
    "# mecab の初期化\n",
    "tagger = MeCab.Tagger(\"-r ../tools/usr/local/etc/mecabrc --unk-feature 未知語\")\n",
    "\n",
    "# 単語頻度辞書の初期化\n",
    "word_counts = defaultdict(lambda: 0)\n",
    "\n",
    "# 抽出語情報リストの初期化\n",
    "words = []\n",
    "\n",
    "# 半角->全角変換マクロを定義する\n",
    "ZEN = \"\".join(chr(0xff01 + i) for i in range(94))\n",
    "HAN = \"\".join(chr(0x21 + i) for i in range(94))\n",
    "HAN2ZEN = str.maketrans(HAN, ZEN)\n",
    "\n",
    "# ストップワードを定義する\n",
    "# stopwords = ['する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう', 'なる', 'ほう']\n",
    "stopwords = [\"湯畑\"]\n",
    "\n",
    "# データ1行ごとのループ\n",
    "for index, row in all_df.iterrows():\n",
    "\n",
    "    # 半角->全角変換した後で, mecab で解析する\n",
    "    node = tagger.parseToNode(row[\"コメント\"].translate(HAN2ZEN))\n",
    "\n",
    "    # 形態素ごとのループ\n",
    "    while node:\n",
    "        # 解析結果を要素ごとにバラす\n",
    "        features = node.feature.split(',')\n",
    "\n",
    "        # 品詞1 を取り出す\n",
    "        pos1 = features[0]\n",
    "\n",
    "        # 品詞2 を取り出す\n",
    "        pos2 = features[1] if len(features) > 1 else \"\"\n",
    "\n",
    "        # 原形 を取り出す\n",
    "        base = features[6] if len(features) > 6 else None\n",
    "\n",
    "        # 原型がストップワードに含まれない単語のみ抽出する\n",
    "        if base not in stopwords:\n",
    "\n",
    "            # 「名詞-一般」\n",
    "            if (pos1 == \"名詞\" and pos2 == \"一般\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"名詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「形容動詞」\n",
    "            elif (pos1 == \"名詞\" and pos2 == \"形容動詞語幹\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                base = f\"{base}\"\n",
    "                postag = \"形容動詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「形容詞」\n",
    "            elif pos1 == \"形容詞\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"形容詞\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            # 「未知語」\n",
    "            elif pos1 == \"未知語\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"未知語\"\n",
    "                key = (base, postag)\n",
    "\n",
    "                # 単語頻度辞書をカウントアップする\n",
    "                word_counts[key] += 1\n",
    "\n",
    "                # 抽出語情報をリストに追加する\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "        # 次の形態素へ\n",
    "        node = node.next\n",
    "\n",
    "# DataFrme 型に整える\n",
    "columns = [\n",
    "    \"文書ID\",\n",
    "    # \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"品詞\",\n",
    "    \"カテゴリー\",\n",
    "    \"エリア\",\n",
    "    \"dict_key\",\n",
    "]\n",
    "docs_df = pd.DataFrame(words, columns=columns)\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(docs_df.shape)\n",
    "display(docs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acea1bc",
   "metadata": {},
   "source": [
    "抽出語の出現頻度をカウントする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「文書-抽出語」 表から単語の出現回数をカウントする\n",
    "word_list = []\n",
    "for i, (k, v) in enumerate(sorted(word_counts.items(), key=lambda x:x[1], reverse=True)):\n",
    "    word_list.append((i, k[0], v, k))\n",
    "\n",
    "# DataFrame 型に整える\n",
    "columns = [\n",
    "    \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"出現頻度\",\n",
    "    \"dict_key\"\n",
    "]\n",
    "\n",
    "# DataFrame を表示する\n",
    "word_counts_df = pd.DataFrame(word_list, columns=columns)\n",
    "print(word_counts_df.shape)\n",
    "display(word_counts_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ec9a1",
   "metadata": {},
   "source": [
    "### 1.1 カテゴリーやエリアごとのユーザーの注目ポイントを押さえる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787d1e6",
   "metadata": {},
   "source": [
    "#### 2.1.1 「文書-抽出語」表の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31148e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「単語出現回数」 表から出現回数Top 1000語のみ抽出する\n",
    "word_counts_1000_df = word_counts_df[0:1000]\n",
    "\n",
    "# 「文書-抽出語」 表も出現回数Top 150語のみに絞り込む\n",
    "merged_df = pd.merge(docs_df, word_counts_1000_df, how=\"inner\", on=\"dict_key\", suffixes=[\"\", \"_right\"])\n",
    "docs_1000_df = merged_df[[\"文書ID\", \"単語ID\", \"表層\", \"品詞\", \"カテゴリー\", \"エリア\", \"dict_key\"]]\n",
    "\n",
    "# 「カテゴリー,エリア」でクロス集計する\n",
    "cross_1000_df = pd.crosstab(\n",
    "    [\n",
    "        docs_1000_df['カテゴリー'], \n",
    "        docs_1000_df['エリア'], \n",
    "        docs_1000_df['文書ID']\n",
    "    ],\n",
    "    docs_1000_df['単語ID'], margins=False\n",
    ")\n",
    "cross_1000_df.columns = word_counts_1000_df[\"表層\"]\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(cross_1000_df.shape)\n",
    "display(cross_1000_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4bedc2",
   "metadata": {},
   "source": [
    "「文書-抽出語」表を {0,1} に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5417b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「文書-抽出語」 表を {0,1} に変換する\n",
    "cross_1000_df[cross_1000_df > 0] = 1\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(cross_1000_df.shape)\n",
    "display(cross_1000_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a445d8f",
   "metadata": {},
   "source": [
    "#### 2.1.2 共起行列を作成する (外部変数-抽出語)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「カテゴリー」のクロス集計と「エリア」のクロス集計を連結する\n",
    "aggregate_df = pd.concat(\n",
    "    [\n",
    "        cross_1000_df.groupby(level='カテゴリー').sum(),\n",
    "        cross_1000_df.groupby(level='エリア').sum()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(aggregate_df.shape)\n",
    "display(aggregate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025980b",
   "metadata": {},
   "source": [
    "#### 2.1.3 Jaccard 係数を求める (外部変数-抽出語)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出語の出現回数を取得する\n",
    "word_counts = cross_1000_df.sum(axis=0).values\n",
    "\n",
    "# 属性(外部変数)出現数を取得する\n",
    "attr_counts = np.hstack(\n",
    "    [\n",
    "        all_df.value_counts('カテゴリー').values,\n",
    "        all_df.value_counts('エリア').values\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 共起行列の中身を Jaccard 係数に入れ替える\n",
    "jaccard_attrs_df = gssm_utils.jaccard_attrs_coef(aggregate_df, attr_counts, word_counts, total=10000, conditional=False)\n",
    "\n",
    "# DataFrame を表示する\n",
    "print(jaccard_attrs_df.shape)\n",
    "display(jaccard_attrs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed2dae",
   "metadata": {},
   "source": [
    "#### 2.1.4 特徴語ランキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「カテゴリ」や「エリア」ごとに Jaccard 係数のTop 10語を抽出する\n",
    "df_list = []\n",
    "for index, row in jaccard_attrs_df.iterrows():\n",
    "    df_list.append(row.iloc[np.argsort(row.values)[::-1][:10]].reset_index())\n",
    "\n",
    "# 「カテゴリ」や「エリア」ごとの Jaccard 係数のTop 10 を横方向に連結した DetaFrame を作成する\n",
    "ranking_df = pd.DataFrame(pd.concat(df_list, axis=1))\n",
    "ranking_df.columns = np.array([c for pair in [[x,f\"jaccard\"] for x in jaccard_attrs_df.index] for c in pair], dtype='object')\n",
    "\n",
    "# DataFrame を表示する\n",
    "display(ranking_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815a9f0",
   "metadata": {},
   "source": [
    "ファイルに出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08073bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_df.to_csv(\"practice-4.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33eb5b8",
   "metadata": {},
   "source": [
    "#### 2.1.5 ワードクラウド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# サブルーチン\n",
    "def sort_and_plot(name, group):\n",
    "\n",
    "    # 「カテゴリー」ごとに Jaccard 係数でソートする\n",
    "    sorted_columns = np.argsort(jaccard_attrs_df.loc[name].values)[::-1][:75]\n",
    "\n",
    "    # Jaccard 係数Top 75語をソートして抽出する\n",
    "    group_cross_df = group.iloc[:,sorted_columns]\n",
    "\n",
    "    # プロットする\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    gssm_utils.plot_wordcloud_ax(ax, \" \".join(group_cross_df.columns))\n",
    "    ax.set_title(name)\n",
    "\n",
    "\n",
    "# プロットの準備\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "i = 0\n",
    "# カテゴリごとのループ\n",
    "for name, group in cross_1000_df.groupby(level='カテゴリー'):\n",
    "    # サブルーチンを呼ぶ\n",
    "    sort_and_plot(name, group)\n",
    "    i += 1\n",
    "\n",
    "    # エリアごとのループ\n",
    "    for sub_name, sub_group in group.groupby(level='エリア'):\n",
    "        # サブルーチンを呼ぶ\n",
    "        sort_and_plot(sub_name, sub_group)\n",
    "        i += 1\n",
    "\n",
    "# プロットの仕上げ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c023d1",
   "metadata": {},
   "source": [
    "#### 2.1.6 共起ネットワーク図 (カテゴリ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出語の出現回数を取得する\n",
    "word_counts = cross_1000_df.sum(axis=0).values\n",
    "\n",
    "# 属性(外部変数)出現数を取得する\n",
    "attr_counts = np.hstack(\n",
    "    [\n",
    "        all_df.value_counts('カテゴリー').values,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# カテゴリのみの共起行列(共起度数)を取得する\n",
    "df = aggregate_df.loc[[\"A_レジャー\",\"B_ビジネス\"],:]\n",
    "\n",
    "# 共起行列(共起度数)で共起ネットワーク図を作成する\n",
    "gssm_utils.plot_attrs_network(df, attr_counts, word_counts, np.sort(df.values.reshape(-1))[::-1][60], width=8, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e61e76",
   "metadata": {},
   "source": [
    "#### 2.1.7 共起ネットワーク図 (エリア)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出語の出現回数を取得する\n",
    "word_counts = cross_1000_df.sum(axis=0).values\n",
    "\n",
    "# 属性(外部変数)出現数を取得する\n",
    "attr_counts = np.hstack(\n",
    "    [\n",
    "        all_df.value_counts('エリア').values,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# カテゴリのみの共起行列(共起度数)を取得する\n",
    "df = aggregate_df.iloc[2:,:]\n",
    "\n",
    "# 共起行列((共起度数)で共起ネットワーク図を作成する\n",
    "gssm_utils.plot_attrs_network(df, attr_counts, word_counts, np.sort(df.values.reshape(-1))[::-1][120], width=8, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5672dd",
   "metadata": {},
   "source": [
    "#### 2.1.8 トピックを抽出する (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリのインポート\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# ライブラリ LDA によるトピック抽出\n",
    "lda = LDA(max_iter=25, learning_method='batch', random_state=0, n_jobs=-1, n_components=6)\n",
    "lda.fit(cross_1000_df.values)\n",
    "\n",
    "# トピックごとに出現確率Top 20語を表示する\n",
    "n_top_words = 20\n",
    "feature_names = cross_1000_df.columns\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic # {topic_idx+1}:\", end=\" \")\n",
    "    for i in topic.argsort()[:-n_top_words-1:-1]:\n",
    "        print(feature_names[i], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815bde4",
   "metadata": {},
   "source": [
    "ChatGPT を使ってトピックを説明する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54008b06",
   "metadata": {},
   "source": [
    "プロンプトの例:\n",
    "> 以下はトピックとトピックごとの高確率ワードです. これを読んで各トピックの要約を日本語で作成してください.\n",
    "> \n",
    "> Topic # 1\tフロント ホテル 浴場 部屋 親切だ 良い …"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcd7af",
   "metadata": {},
   "source": [
    "結果の例:\n",
    "- トピック 1: ホテルの立地と利便性\n",
    "- トピック 2: ホテルの食事とスタッフのサービス\n",
    "- トピック 3: ホテルの部屋と風呂\n",
    "- トピック 4: 温泉と風呂\n",
    "- トピック 5: ホテルの全体的な評価\n",
    "- トピック 6: 子連れ家族の滞在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックごとに出現確率Top 75語をプロットする\n",
    "n_top_words = 75\n",
    "gssm_utils.plot_topic_model(lda, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd25701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# サブルーチン\n",
    "def sort_and_plot(name, group):\n",
    "\n",
    "    # 文書ごとのトピック比率を取得\n",
    "    doc_topic_distributions = lda.transform(group.values)\n",
    "\n",
    "    # 文書全体のトピック比率を計算（平均を取る）\n",
    "    overall_topic_distribution = np.mean(doc_topic_distributions, axis=0)\n",
    "\n",
    "    # プロットする\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    gssm_utils.plot_topic_distribution_ax(ax, overall_topic_distribution)\n",
    "    ax.set_title(name)\n",
    "\n",
    "# プロットの準備\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "i = 0\n",
    "# カテゴリごとのループ\n",
    "for name, group in cross_1000_df.groupby(level='カテゴリー'):\n",
    "    # サブルーチンを呼ぶ\n",
    "    sort_and_plot(name, group)\n",
    "    i += 1\n",
    "\n",
    "    # エリアごとのループ\n",
    "    for sub_name, sub_group in group.groupby(level='エリア'):\n",
    "        # サブルーチンを呼ぶ\n",
    "        sort_and_plot(sub_name, sub_group)\n",
    "        i += 1\n",
    "\n",
    "# プロットの仕上げ\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gssm2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
